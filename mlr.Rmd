---
title: "mlr"
author: "Hao Zheng"
date: "12/12/2021"
output: github_document
---

```{r}
library(tidyverse)
library(modelr)
library(patchwork)

cdi = read.csv("./data/cdi.csv")
```

## Multiple Linear Regression Model

Next, we try to fit a multiple regression model over the variables, with the newly created variable crm_1000 as the outcome.
```{r}
# Create a new variable called crm_1000, which is the crime rate per 1000 population in each county in year 1990, and another variable poparea which is the population density per square mile. Also change the number of doctors and beds into doctors and beds per 1000 population.

cdi = 
  cdi %>% 
  mutate(
    # crm_1000 is already generated in Xiao's part
    crm_1000 = crimes/pop * 1000,
    poparea = pop/area,
    docs = docs/pop * 1000,
    beds = beds/pop * 1000,
    # mutatation for the region needs to stay here (not in Xiao's part)
    region = as.factor(region)
  ) %>% 
  select(-id,-cty,-state,-crimes)
```

### Model 1: Full model
Let's first fit the model with all the predictors:
```{r}
model1 = lm(crm_1000 ~ ., data = cdi)

broom::tidy(model1) %>% 
  knitr::kable()
```

We can see that the variables `area`, `pop65`, `docs`, `bagrad`, `unemp` are all not very significant with a p-value larger than 0.05.


### Model 2: Model found by looking at correlation clusters
Then, plot a heatmap for the correlations among all the variables.

```{r}
res = cor(cdi %>% select(-region))
round(res, 2) %>%
  knitr::kable()

# plot the heatmap
col<- colorRampPalette(c("blue", "white", "red"))(20)
heatmap(x = res, col = col, symm = TRUE)
```

According to the clusters generated by R, we then choose variables that are not highly related with each other and are highly related to the outcome `crm_1000`.

```{r}
model2 = lm(crm_1000 ~ pop18 + pcincome + hsgrad + pop65 + poverty + beds + poparea + region, data = cdi)

broom::tidy(model2) %>% 
  knitr::kable()
```


### Model 3: Step-wise model based on model 2

Conduct the automatic step-wise process on the predictors used in model 2.
```{r}
mult.fit = lm(crm_1000 ~ pop18 + pcincome + hsgrad + pop65 + poverty + beds + poparea + region, data = cdi)
step(mult.fit, direction = "both")

# Obtain a new model 3
model3 = lm(crm_1000 ~ pop18 + pcincome + poverty + beds + poparea + region, data = cdi)

broom::tidy(model3) %>% 
  knitr::kable()
```

Model 3 is rather good, and with a smaller number of predictors.

Then we need to see the residual plot for model 3 in order to get some insight of some potential transformation.
```{r}
cdi %>% 
  add_predictions(model3) %>% 
  add_residuals(model3) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(se = F, color = "red", method = "lm") +
  labs(title = "Risidual Plot for model 3",
       x = "Fitted Value", 
       y = "Residual")
```

The residuals are randomly scattered so we may suppose that linearity has already been achieved.


### Model 4: Try to add interaction terms in model 3

We can then consider adding some potential interaction terms to model 3. So what interaction terms can we add?

We suppose the terms that both have significant main effect on the outcomes may interact together on the outcomes, so we add the pairwise interactions of `poparea`, `poverty`, `region` and `beds` into model 3. Next, Apply step-wise process on it to come up with a new model.
```{r}
mult.fit = lm(crm_1000 ~ pop18 + pcincome + poverty + beds + poparea + region + poparea*poverty + poparea*region + poverty*region + beds*poparea + beds*region + beds*poverty, data = cdi)
step(mult.fit, direction = "both")

model4 = lm(crm_1000 ~ pop18 + pcincome + poverty + beds + poparea + region + poparea*region + poparea*poverty + poparea*beds, data = cdi)

broom::tidy(model4) %>% 
  knitr::kable()
```


### Model 5 & 6: Model based on internet searching result
In addition, we can try to find other models based on the searching results of the potentially related factors of crime rates: age, medical resources, educational level, poverty, unemployment rate, personal income and population density per square mile.

```{r}
model5 = lm(crm_1000 ~ pop18 + beds + hsgrad + poverty + unemp + pcincome + poparea, data = cdi)

broom::tidy(model5) %>% 
  knitr::kable()
```

Then we use step-wise process based on the predictors used in model 4.

```{r}
mult.fit = lm(crm_1000 ~ pop18 + beds + hsgrad + poverty + unemp + pcincome + poparea, data = cdi)
step(mult.fit, direction = "both")

model6 = lm(crm_1000 ~ pop18 + beds + poverty + unemp + pcincome + poparea, data = cdi)

broom::tidy(model6) %>% 
  knitr::kable()
```

Draw the residual plots:
```{r}
res5 = 
  cdi %>% 
  add_predictions(model5) %>% 
  add_residuals(model5) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(se = F, color = "red", method = "lm") +
  labs(title = "Risidual Plot for model 5",
       x = "Fitted Value", 
       y = "Residual")

res6 = 
  cdi %>% 
  add_predictions(model6) %>% 
  add_residuals(model6) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.4) + 
  geom_smooth(se = F, color = "red", method = "lm") +
  labs(title = "Risidual Plot for model 6",
       x = "Fitted Value", 
       y = "Residual")

res5 + res6
```

By the residual plot for the above two models, we cannot see a big difference.



### Model 7: Step-wise model generated with all the variables

```{r}
# Use step-wise regression to try to find a mlr model
mult.fit = lm(crm_1000 ~ ., data = cdi)
step(mult.fit, direction = "both")
```

According the above results of step-wise regression in R, the predicting model of `crm_1000` contains the continuous predictors `pop`, `pop18`, `beds`, `hsgrad`, `bagrad`, `poverty`, `pcincome`, `totalinc`, `poparea` and the categorical predictor `region`.

Then we fit the multiple linear regression model 6 for crime rates:
```{r}
model7 = lm(crm_1000 ~ pop + pop18 + beds + hsgrad + bagrad + poverty + pcincome + totalinc + poparea + region, data = cdi)

broom::tidy(model7) %>% 
  knitr::kable()
```


So we try to compare the models by ploting their RMSE values.
```{r}
# I've only chose model 3, 4, 7 here to compare their RMSE values
cv = crossv_mc(cdi, 100) %>% 
    mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% 
  mutate(
    model_3 = map(train, ~lm(crm_1000 ~ pop18 + pcincome + poverty + beds + poparea + region, data = cdi)),
    model_4 = map(train, ~lm(crm_1000 ~ pop18 + pcincome + poverty + beds + poparea + region + poparea*region + poparea*poverty + poparea*beds, data = cdi)),
    model_7 = map(train, ~lm(crm_1000 ~ pop + pop18 + beds + hsgrad + bagrad + poverty + pcincome + totalinc + poparea + region, data = cdi))) %>% 
  mutate(
    rmse_model3 = map2_dbl(model_3, test, ~rmse(model = .x, data = .y)),
    rmse_model4 = map2_dbl(model_4, test, ~rmse(model = .x, data = .y)),
    rmse_model7 = map2_dbl(model_7, test, ~rmse(model = .x, data = .y))
    )

cv %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_" 
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +  geom_violin(fill = "orange",alpha = 0.4) +
  geom_boxplot(alpha = 0.5, color = "red") 

```